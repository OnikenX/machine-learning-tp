{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados\n",
    "Foi se usado os dados que foram processados na meta anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "redwine_and_whitewine = pd.read_csv('merged.csv', sep=',')\n",
    "X = redwine_and_whitewine.drop(columns='Red_wine')\n",
    "y = redwine_and_whitewine['Red_wine']\n",
    "X_train , X_test, y_train , y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo a usar\n",
    "\n",
    "Depois de lido o [este artigo](https://medium.com/thinkport/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2), dicidiu se usar o **VotingClassifier** para e os vários algoritmos também testados sozinhos, estes sendo o **naive bias**, **logistic regression**, **random forest** e o **vector machine**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.9097744360902256\n",
      "score on train: 0.91624895572264\n"
     ]
    }
   ],
   "source": [
    "#inclusão do naive bias\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB().fit(X_train, y_train)\n",
    "print(\"score on test: \" + str(mnb.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(mnb.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.9830827067669173\n",
      "score on train: 0.9849624060150376\n"
     ]
    }
   ],
   "source": [
    "#inclusão do logistic regression com uma iteração maxima de 1000\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "print(\"score on test: \" + str(lr.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(lr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.9906015037593985\n",
      "score on train: 0.9985380116959064\n"
     ]
    }
   ],
   "source": [
    "#inclusão do Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# n_estimators = number of decision trees\n",
    "rf = RandomForestClassifier(n_estimators=30, max_depth=9).fit(X_train, y_train)\n",
    "print(\"score on test: \" + str(rf.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(rf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.9342105263157895\n",
      "score on train: 0.9398496240601504\n"
     ]
    }
   ],
   "source": [
    "#finalmente inclusão do vector machine com o parapetro de regularização a 0.0001 sendo este o recomendado \n",
    "from sklearn.svm import LinearSVC\n",
    "svm=LinearSVC(C=0.0001).fit(X_train, y_train)\n",
    "print(\"score on test: \" + str(svm.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(svm.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test: 0.956766917293233\n",
      "score on train: 0.9582289055973267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# 1) naive bias = mnb\n",
    "mnb = MultinomialNB()\n",
    "# 2) logistic regression =lr\n",
    "lr=LogisticRegression(max_iter=1000)\n",
    "# 3) random forest =rf\n",
    "rf = RandomForestClassifier(n_estimators=30, max_depth=9)\n",
    "# 4) support vector machine = svm\n",
    "svm=LinearSVC(C=0.0001)\n",
    "\n",
    "evc=VotingClassifier(estimators=[('mnb',mnb),('lr',lr),('rf',rf),('svm',svm)],voting='hard')\n",
    "\n",
    "\n",
    "evc.fit(X_train, y_train)\n",
    "print(\"score on test: \" + str(evc.score(X_test, y_test)))\n",
    "print(\"score on train: \"+ str(evc.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de executados todos os algoritmos o que mostrou melhores resultados para o caso em questão foi o **RandomForestClassifier** com o menor tempo, 0.2s e com os melhores resultados, 99% de *score*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
